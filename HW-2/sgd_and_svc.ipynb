{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a20570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./Data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcf9e22b18e49eb93d4ac07dc2d8c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./Data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./Data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16660bc17a5340cf8282a925efdfe7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./Data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./Data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdf3135ac084eee85195320c2359ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./Data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./Data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef00667fc3d442f982408cfb4359031c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./Data\\MNIST\\raw\n",
      "\n",
      "---SGD without regularization---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cagla demirhan\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration_noation number: 500. Loss value: 1.928480625152588. Accuracy: 66.81\n",
      "iteration_noation number: 1000. Loss value: 1.5894123315811157. Accuracy: 75.84\n",
      "iteration_noation number: 1500. Loss value: 1.3410272598266602. Accuracy: 78.76\n",
      "iteration_noation number: 2000. Loss value: 1.218279480934143. Accuracy: 80.59\n",
      "iteration_noation number: 2500. Loss value: 1.1146430969238281. Accuracy: 81.55\n",
      "iteration_noation number: 3000. Loss value: 1.0389161109924316. Accuracy: 82.3\n",
      "iteration_noation number: 3500. Loss value: 0.8829132318496704. Accuracy: 83.02\n",
      "iteration_noation number: 4000. Loss value: 0.945344865322113. Accuracy: 83.65\n",
      "iteration_noation number: 4500. Loss value: 0.8835456371307373. Accuracy: 83.98\n",
      "iteration_noation number: 5000. Loss value: 0.8420906662940979. Accuracy: 84.51\n",
      "iteration_noation number: 5500. Loss value: 0.8039150834083557. Accuracy: 84.82\n",
      "iteration_noation number: 6000. Loss value: 0.6774627566337585. Accuracy: 85.13\n",
      "iteration_noation number: 6500. Loss value: 0.6896381378173828. Accuracy: 85.5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# author: @emrenuray\n",
    "\n",
    "# SGD Practical\n",
    "# No regularization\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "#Fetching data\n",
    "train_data = dsets.MNIST(root='./Data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = dsets.MNIST(root='./Data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "#Defining sample set to use for each gradient update and other iteration_noation, epoch parameters\n",
    "batch = 100\n",
    "iteration_noations = 7000\n",
    "epochs = int(iteration_noations / (len(train_data) / batch))\n",
    "\n",
    "#Loading data\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch, shuffle=False)\n",
    "\n",
    "#Function for Linear Regrassion model\n",
    "class LRModel(nn.Module):    \n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LRModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "#Defining dimensions for linear reg.  \n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "\n",
    "#Creating model\n",
    "model = LRModel(input_dim, output_dim)\n",
    "\n",
    "#Defining torch device\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#Determining the evaluation criteration_noia for Loss\n",
    "criteration_noion = nn.CrossEntropyLoss()\n",
    "\n",
    "#For no regularization, no weight decay parameter was added to the optimization function.\n",
    "optimizer_no_reg = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"---SGD without regularization---\")\n",
    "\n",
    "iteration_no = 0\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #Assignment of images and labels.\n",
    "        images = images.view(-1, 28*28).requires_grad_().to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Set the gradients of all optimized tensors to zero\n",
    "        optimizer_no_reg.zero_grad()\n",
    "\n",
    "        results = model(images)\n",
    "        \n",
    "        #Computing cross entropy loss\n",
    "        loss = criteration_noion(results, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        #Applying parameter update\n",
    "        optimizer_no_reg.step()\n",
    "\n",
    "        iteration_no += 1\n",
    "        if iteration_no % 500 == 0:         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            #Calculating accuracy\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28*28).to(device)\n",
    "                results = model(images)\n",
    "                _, predicted = torch.max(results.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "            print('iteration_noation number: {}. Loss value: {}. Accuracy: {}'.format(iteration_no, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b5e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---SGD with regularization---\n",
      "iteration_noation number: 500. Loss value: 0.6812131404876709. Accuracy: 85.77\n",
      "iteration_noation number: 1000. Loss value: 0.6399814486503601. Accuracy: 86.04\n",
      "iteration_noation number: 1500. Loss value: 0.7117151618003845. Accuracy: 86.24\n",
      "iteration_noation number: 2000. Loss value: 0.7015759944915771. Accuracy: 86.47\n",
      "iteration_noation number: 2500. Loss value: 0.7219719886779785. Accuracy: 86.55\n",
      "iteration_noation number: 3000. Loss value: 0.7199192047119141. Accuracy: 86.6\n",
      "iteration_noation number: 3500. Loss value: 0.6077828407287598. Accuracy: 86.69\n",
      "iteration_noation number: 4000. Loss value: 0.49089232087135315. Accuracy: 86.78\n",
      "iteration_noation number: 4500. Loss value: 0.5951270461082458. Accuracy: 86.99\n"
     ]
    }
   ],
   "source": [
    "# Regularization\n",
    "\n",
    "print(\"---SGD with regularization---\")\n",
    "optimizer_w_reg = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "\n",
    "iteration_no = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.view(-1, 28*28).requires_grad_().to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer_w_reg.zero_grad()\n",
    "\n",
    "        results = model(images)\n",
    "        \n",
    "        loss = criteration_noion(results, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_w_reg.step()\n",
    "\n",
    "        iteration_no += 1\n",
    "        if iteration_no % 500 == 0:         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28*28).to(device)\n",
    "                results = model(images)\n",
    "                _, predicted = torch.max(results.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "            print('iteration_noation number: {}. Loss value: {}. Accuracy: {}'.format(iteration_no, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f716ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical SVC\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Fetching data\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X /255.0\n",
    "\n",
    "#Splitting dataset\n",
    "X_train, X_test = X[:1000], X[1000:]\n",
    "y_train, y_test = y[:1000], y[1000:]\n",
    "\n",
    "#Modelling SVM with regularization\n",
    "SVC_w_reg = SVC(C=10, gamma=0.001, kernel=\"rbf\") #rbf=radial basis function, search it!\n",
    "SVC_w_reg.fit(X_train, y_train)\n",
    "\n",
    "#Predicting with reg.\n",
    "y_pred_reg = SVC_w_reg.predict(X_test)\n",
    "\n",
    "#Accuracy I\n",
    "print(\"Acccuracy of SVC Model with regularization : \", metrics.accuracy_score(y_test, y_pred_reg))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "#Modelling SVM without regularization\n",
    "SVC_no_reg = SVC(C=1000, gamma=0.001, kernel=\"rbf\")\n",
    "SVC_no_reg.fit(X_train, y_train)\n",
    "\n",
    "#Predicting without reg.\n",
    "y_pred_no_reg = SVC_no_reg.predict(X_test)\n",
    "\n",
    "#Accuracy II\n",
    "print(\"Acccuracy of SVC Model without regularization : \", metrics.accuracy_score(y_test, y_pred_no_reg))\n",
    "\n",
    "#Difference\n",
    "print(((metrics.accuracy_score(y_test, y_pred_reg))-(metrics.accuracy_score(y_test, y_pred_no_reg)))/(metrics.accuracy_score(y_test, y_pred_reg))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49f0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
